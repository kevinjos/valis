{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhttps://www.tensorflow.org/tutorials/text/transformer\\n1. Byte-Pair encode the corpus\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "https://www.tensorflow.org/tutorials/text/transformer\n",
    "1. Byte-Pair encode the corpus\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n",
    "                               as_supervised=True)\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n",
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n",
      "7877 ----> .\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "assert original_string == sample_string\n",
    "\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "      lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "  \n",
    "    return lang1, lang2\n",
    "\n",
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                        tf.size(y) <= max_length)\n",
    "\n",
    "def tf_encode(pt, en):\n",
    "    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "# cache the dataset to memory to get a speedup while reading from it.\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=207688, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8214, 1259,    5, ...,    0,    0,    0],\n",
       "        [8214,  299,   13, ...,    0,    0,    0],\n",
       "        [8214,   59,    8, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,   95,    3, ...,    0,    0,    0],\n",
       "        [8214, 5157,    1, ...,    0,    0,    0],\n",
       "        [8214, 4479, 7990, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: id=207689, shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8087,   18,   12, ...,    0,    0,    0],\n",
       "        [8087,  634,   30, ...,    0,    0,    0],\n",
       "        [8087,   16,   13, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8087,   12,   20, ...,    0,    0,    0],\n",
       "        [8087,   17, 4981, ...,    0,    0,    0],\n",
       "        [8087,   12, 5453, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.4650 Accuracy 0.3442\n",
      "Epoch 1 Batch 50 Loss 0.5060 Accuracy 0.3552\n",
      "Epoch 1 Batch 100 Loss 0.5035 Accuracy 0.3542\n",
      "Epoch 1 Batch 150 Loss 0.5109 Accuracy 0.3528\n",
      "Epoch 1 Batch 200 Loss 0.5124 Accuracy 0.3515\n",
      "Epoch 1 Batch 250 Loss 0.5138 Accuracy 0.3504\n",
      "Epoch 1 Batch 300 Loss 0.5172 Accuracy 0.3496\n",
      "Epoch 1 Batch 350 Loss 0.5221 Accuracy 0.3491\n",
      "Epoch 1 Batch 400 Loss 0.5244 Accuracy 0.3486\n",
      "Epoch 1 Batch 450 Loss 0.5264 Accuracy 0.3485\n",
      "Epoch 1 Batch 500 Loss 0.5297 Accuracy 0.3482\n",
      "Epoch 1 Batch 550 Loss 0.5327 Accuracy 0.3475\n",
      "Epoch 1 Batch 600 Loss 0.5348 Accuracy 0.3469\n",
      "Epoch 1 Batch 650 Loss 0.5370 Accuracy 0.3464\n",
      "Epoch 1 Batch 700 Loss 0.5403 Accuracy 0.3464\n",
      "Epoch 1 Loss 0.5404 Accuracy 0.3463\n",
      "Time taken for 1 epoch: 111.03527855873108 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.4662 Accuracy 0.3466\n",
      "Epoch 2 Batch 50 Loss 0.4747 Accuracy 0.3542\n",
      "Epoch 2 Batch 100 Loss 0.4817 Accuracy 0.3556\n",
      "Epoch 2 Batch 150 Loss 0.4858 Accuracy 0.3531\n",
      "Epoch 2 Batch 200 Loss 0.4913 Accuracy 0.3550\n",
      "Epoch 2 Batch 250 Loss 0.4948 Accuracy 0.3525\n",
      "Epoch 2 Batch 300 Loss 0.4986 Accuracy 0.3524\n",
      "Epoch 2 Batch 350 Loss 0.5020 Accuracy 0.3519\n",
      "Epoch 2 Batch 400 Loss 0.5054 Accuracy 0.3518\n",
      "Epoch 2 Batch 450 Loss 0.5091 Accuracy 0.3512\n",
      "Epoch 2 Batch 500 Loss 0.5106 Accuracy 0.3504\n",
      "Epoch 2 Batch 550 Loss 0.5131 Accuracy 0.3501\n",
      "Epoch 2 Batch 600 Loss 0.5163 Accuracy 0.3493\n",
      "Epoch 2 Batch 650 Loss 0.5197 Accuracy 0.3490\n",
      "Epoch 2 Batch 700 Loss 0.5232 Accuracy 0.3491\n",
      "Epoch 2 Loss 0.5234 Accuracy 0.3492\n",
      "Time taken for 1 epoch: 62.72203302383423 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.4389 Accuracy 0.3473\n",
      "Epoch 3 Batch 50 Loss 0.4584 Accuracy 0.3555\n",
      "Epoch 3 Batch 100 Loss 0.4702 Accuracy 0.3569\n",
      "Epoch 3 Batch 150 Loss 0.4757 Accuracy 0.3571\n",
      "Epoch 3 Batch 200 Loss 0.4776 Accuracy 0.3552\n",
      "Epoch 3 Batch 250 Loss 0.4785 Accuracy 0.3538\n",
      "Epoch 3 Batch 300 Loss 0.4831 Accuracy 0.3547\n",
      "Epoch 3 Batch 350 Loss 0.4873 Accuracy 0.3547\n",
      "Epoch 3 Batch 400 Loss 0.4907 Accuracy 0.3549\n",
      "Epoch 3 Batch 450 Loss 0.4936 Accuracy 0.3548\n",
      "Epoch 3 Batch 500 Loss 0.4970 Accuracy 0.3546\n",
      "Epoch 3 Batch 550 Loss 0.4995 Accuracy 0.3540\n",
      "Epoch 3 Batch 600 Loss 0.5021 Accuracy 0.3539\n",
      "Epoch 3 Batch 650 Loss 0.5048 Accuracy 0.3537\n",
      "Epoch 3 Batch 700 Loss 0.5067 Accuracy 0.3527\n",
      "Epoch 3 Loss 0.5066 Accuracy 0.3526\n",
      "Time taken for 1 epoch: 62.79980826377869 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.5084 Accuracy 0.3690\n",
      "Epoch 4 Batch 50 Loss 0.4395 Accuracy 0.3555\n",
      "Epoch 4 Batch 100 Loss 0.4514 Accuracy 0.3556\n",
      "Epoch 4 Batch 150 Loss 0.4582 Accuracy 0.3570\n",
      "Epoch 4 Batch 200 Loss 0.4626 Accuracy 0.3564\n",
      "Epoch 4 Batch 250 Loss 0.4631 Accuracy 0.3562\n",
      "Epoch 4 Batch 300 Loss 0.4666 Accuracy 0.3563\n",
      "Epoch 4 Batch 350 Loss 0.4711 Accuracy 0.3569\n",
      "Epoch 4 Batch 400 Loss 0.4747 Accuracy 0.3568\n",
      "Epoch 4 Batch 450 Loss 0.4789 Accuracy 0.3570\n",
      "Epoch 4 Batch 500 Loss 0.4820 Accuracy 0.3565\n",
      "Epoch 4 Batch 550 Loss 0.4839 Accuracy 0.3559\n",
      "Epoch 4 Batch 600 Loss 0.4871 Accuracy 0.3556\n",
      "Epoch 4 Batch 650 Loss 0.4897 Accuracy 0.3549\n",
      "Epoch 4 Batch 700 Loss 0.4918 Accuracy 0.3544\n",
      "Epoch 4 Loss 0.4919 Accuracy 0.3543\n",
      "Time taken for 1 epoch: 62.75460863113403 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.4768 Accuracy 0.3847\n",
      "Epoch 5 Batch 50 Loss 0.4270 Accuracy 0.3604\n",
      "Epoch 5 Batch 100 Loss 0.4341 Accuracy 0.3620\n",
      "Epoch 5 Batch 150 Loss 0.4399 Accuracy 0.3609\n",
      "Epoch 5 Batch 200 Loss 0.4445 Accuracy 0.3604\n",
      "Epoch 5 Batch 250 Loss 0.4499 Accuracy 0.3608\n",
      "Epoch 5 Batch 300 Loss 0.4535 Accuracy 0.3612\n",
      "Epoch 5 Batch 350 Loss 0.4568 Accuracy 0.3606\n",
      "Epoch 5 Batch 400 Loss 0.4613 Accuracy 0.3603\n",
      "Epoch 5 Batch 450 Loss 0.4638 Accuracy 0.3599\n",
      "Epoch 5 Batch 500 Loss 0.4656 Accuracy 0.3592\n",
      "Epoch 5 Batch 550 Loss 0.4683 Accuracy 0.3588\n",
      "Epoch 5 Batch 600 Loss 0.4711 Accuracy 0.3583\n",
      "Epoch 5 Batch 650 Loss 0.4730 Accuracy 0.3573\n",
      "Epoch 5 Batch 700 Loss 0.4758 Accuracy 0.3566\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
      "Epoch 5 Loss 0.4759 Accuracy 0.3567\n",
      "Time taken for 1 epoch: 63.0560736656189 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.4185 Accuracy 0.3885\n",
      "Epoch 6 Batch 50 Loss 0.4316 Accuracy 0.3667\n",
      "Epoch 6 Batch 100 Loss 0.4298 Accuracy 0.3655\n",
      "Epoch 6 Batch 150 Loss 0.4316 Accuracy 0.3652\n",
      "Epoch 6 Batch 200 Loss 0.4350 Accuracy 0.3656\n",
      "Epoch 6 Batch 250 Loss 0.4393 Accuracy 0.3656\n",
      "Epoch 6 Batch 300 Loss 0.4419 Accuracy 0.3633\n",
      "Epoch 6 Batch 350 Loss 0.4453 Accuracy 0.3632\n",
      "Epoch 6 Batch 400 Loss 0.4487 Accuracy 0.3629\n",
      "Epoch 6 Batch 450 Loss 0.4515 Accuracy 0.3626\n",
      "Epoch 6 Batch 500 Loss 0.4540 Accuracy 0.3620\n",
      "Epoch 6 Batch 550 Loss 0.4558 Accuracy 0.3615\n",
      "Epoch 6 Batch 600 Loss 0.4583 Accuracy 0.3609\n",
      "Epoch 6 Batch 650 Loss 0.4611 Accuracy 0.3602\n",
      "Epoch 6 Batch 700 Loss 0.4638 Accuracy 0.3595\n",
      "Epoch 6 Loss 0.4640 Accuracy 0.3595\n",
      "Time taken for 1 epoch: 62.826725482940674 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.4352 Accuracy 0.3865\n",
      "Epoch 7 Batch 50 Loss 0.4165 Accuracy 0.3682\n",
      "Epoch 7 Batch 100 Loss 0.4210 Accuracy 0.3690\n",
      "Epoch 7 Batch 150 Loss 0.4209 Accuracy 0.3681\n",
      "Epoch 7 Batch 200 Loss 0.4249 Accuracy 0.3674\n",
      "Epoch 7 Batch 250 Loss 0.4262 Accuracy 0.3664\n",
      "Epoch 7 Batch 300 Loss 0.4291 Accuracy 0.3657\n",
      "Epoch 7 Batch 350 Loss 0.4324 Accuracy 0.3650\n",
      "Epoch 7 Batch 400 Loss 0.4351 Accuracy 0.3647\n",
      "Epoch 7 Batch 450 Loss 0.4377 Accuracy 0.3642\n",
      "Epoch 7 Batch 500 Loss 0.4407 Accuracy 0.3634\n",
      "Epoch 7 Batch 550 Loss 0.4426 Accuracy 0.3630\n",
      "Epoch 7 Batch 600 Loss 0.4460 Accuracy 0.3625\n",
      "Epoch 7 Batch 650 Loss 0.4488 Accuracy 0.3618\n",
      "Epoch 7 Batch 700 Loss 0.4515 Accuracy 0.3614\n",
      "Epoch 7 Loss 0.4513 Accuracy 0.3614\n",
      "Time taken for 1 epoch: 62.642146587371826 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.4281 Accuracy 0.3702\n",
      "Epoch 8 Batch 50 Loss 0.4030 Accuracy 0.3707\n",
      "Epoch 8 Batch 100 Loss 0.4051 Accuracy 0.3681\n",
      "Epoch 8 Batch 150 Loss 0.4121 Accuracy 0.3705\n",
      "Epoch 8 Batch 200 Loss 0.4135 Accuracy 0.3688\n",
      "Epoch 8 Batch 250 Loss 0.4172 Accuracy 0.3688\n",
      "Epoch 8 Batch 300 Loss 0.4213 Accuracy 0.3688\n",
      "Epoch 8 Batch 350 Loss 0.4246 Accuracy 0.3686\n",
      "Epoch 8 Batch 400 Loss 0.4278 Accuracy 0.3683\n",
      "Epoch 8 Batch 450 Loss 0.4295 Accuracy 0.3674\n",
      "Epoch 8 Batch 500 Loss 0.4307 Accuracy 0.3669\n",
      "Epoch 8 Batch 550 Loss 0.4337 Accuracy 0.3663\n",
      "Epoch 8 Batch 600 Loss 0.4366 Accuracy 0.3657\n",
      "Epoch 8 Batch 650 Loss 0.4392 Accuracy 0.3648\n",
      "Epoch 8 Batch 700 Loss 0.4414 Accuracy 0.3640\n",
      "Epoch 8 Loss 0.4414 Accuracy 0.3640\n",
      "Time taken for 1 epoch: 62.714086294174194 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3992 Accuracy 0.3737\n",
      "Epoch 9 Batch 50 Loss 0.3928 Accuracy 0.3721\n",
      "Epoch 9 Batch 100 Loss 0.3981 Accuracy 0.3749\n",
      "Epoch 9 Batch 150 Loss 0.3992 Accuracy 0.3734\n",
      "Epoch 9 Batch 200 Loss 0.4058 Accuracy 0.3735\n",
      "Epoch 9 Batch 250 Loss 0.4078 Accuracy 0.3715\n",
      "Epoch 9 Batch 300 Loss 0.4096 Accuracy 0.3710\n",
      "Epoch 9 Batch 350 Loss 0.4112 Accuracy 0.3701\n",
      "Epoch 9 Batch 400 Loss 0.4131 Accuracy 0.3688\n",
      "Epoch 9 Batch 450 Loss 0.4155 Accuracy 0.3682\n",
      "Epoch 9 Batch 500 Loss 0.4184 Accuracy 0.3672\n",
      "Epoch 9 Batch 550 Loss 0.4222 Accuracy 0.3666\n",
      "Epoch 9 Batch 600 Loss 0.4245 Accuracy 0.3664\n",
      "Epoch 9 Batch 650 Loss 0.4273 Accuracy 0.3659\n",
      "Epoch 9 Batch 700 Loss 0.4295 Accuracy 0.3657\n",
      "Epoch 9 Loss 0.4296 Accuracy 0.3658\n",
      "Time taken for 1 epoch: 62.95284295082092 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4045 Accuracy 0.4240\n",
      "Epoch 10 Batch 50 Loss 0.3790 Accuracy 0.3765\n",
      "Epoch 10 Batch 100 Loss 0.3783 Accuracy 0.3727\n",
      "Epoch 10 Batch 150 Loss 0.3872 Accuracy 0.3735\n",
      "Epoch 10 Batch 200 Loss 0.3931 Accuracy 0.3727\n",
      "Epoch 10 Batch 250 Loss 0.3967 Accuracy 0.3719\n",
      "Epoch 10 Batch 300 Loss 0.3986 Accuracy 0.3717\n",
      "Epoch 10 Batch 350 Loss 0.4007 Accuracy 0.3709\n",
      "Epoch 10 Batch 400 Loss 0.4026 Accuracy 0.3709\n",
      "Epoch 10 Batch 450 Loss 0.4057 Accuracy 0.3706\n",
      "Epoch 10 Batch 500 Loss 0.4087 Accuracy 0.3703\n",
      "Epoch 10 Batch 550 Loss 0.4112 Accuracy 0.3694\n",
      "Epoch 10 Batch 600 Loss 0.4137 Accuracy 0.3686\n",
      "Epoch 10 Batch 650 Loss 0.4160 Accuracy 0.3679\n",
      "Epoch 10 Batch 700 Loss 0.4192 Accuracy 0.3676\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-6\n",
      "Epoch 10 Loss 0.4193 Accuracy 0.3676\n",
      "Time taken for 1 epoch: 63.24957346916199 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.3301 Accuracy 0.3623\n",
      "Epoch 11 Batch 50 Loss 0.3755 Accuracy 0.3775\n",
      "Epoch 11 Batch 100 Loss 0.3735 Accuracy 0.3756\n",
      "Epoch 11 Batch 150 Loss 0.3787 Accuracy 0.3731\n",
      "Epoch 11 Batch 200 Loss 0.3813 Accuracy 0.3726\n",
      "Epoch 11 Batch 250 Loss 0.3853 Accuracy 0.3720\n",
      "Epoch 11 Batch 300 Loss 0.3897 Accuracy 0.3730\n",
      "Epoch 11 Batch 350 Loss 0.3923 Accuracy 0.3730\n",
      "Epoch 11 Batch 400 Loss 0.3951 Accuracy 0.3726\n",
      "Epoch 11 Batch 450 Loss 0.3969 Accuracy 0.3720\n",
      "Epoch 11 Batch 500 Loss 0.3992 Accuracy 0.3715\n",
      "Epoch 11 Batch 550 Loss 0.4021 Accuracy 0.3713\n",
      "Epoch 11 Batch 600 Loss 0.4037 Accuracy 0.3700\n",
      "Epoch 11 Batch 650 Loss 0.4072 Accuracy 0.3697\n",
      "Epoch 11 Batch 700 Loss 0.4100 Accuracy 0.3691\n",
      "Epoch 11 Loss 0.4104 Accuracy 0.3692\n",
      "Time taken for 1 epoch: 62.903945446014404 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.4007 Accuracy 0.3824\n",
      "Epoch 12 Batch 50 Loss 0.3621 Accuracy 0.3828\n",
      "Epoch 12 Batch 100 Loss 0.3670 Accuracy 0.3791\n",
      "Epoch 12 Batch 150 Loss 0.3734 Accuracy 0.3793\n",
      "Epoch 12 Batch 200 Loss 0.3748 Accuracy 0.3784\n",
      "Epoch 12 Batch 250 Loss 0.3778 Accuracy 0.3771\n",
      "Epoch 12 Batch 300 Loss 0.3797 Accuracy 0.3768\n",
      "Epoch 12 Batch 350 Loss 0.3830 Accuracy 0.3753\n",
      "Epoch 12 Batch 400 Loss 0.3839 Accuracy 0.3746\n",
      "Epoch 12 Batch 450 Loss 0.3865 Accuracy 0.3741\n",
      "Epoch 12 Batch 500 Loss 0.3894 Accuracy 0.3733\n",
      "Epoch 12 Batch 550 Loss 0.3917 Accuracy 0.3723\n",
      "Epoch 12 Batch 600 Loss 0.3942 Accuracy 0.3714\n",
      "Epoch 12 Batch 650 Loss 0.3972 Accuracy 0.3714\n",
      "Epoch 12 Batch 700 Loss 0.3995 Accuracy 0.3710\n",
      "Epoch 12 Loss 0.3996 Accuracy 0.3710\n",
      "Time taken for 1 epoch: 62.68379211425781 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.3535 Accuracy 0.4116\n",
      "Epoch 13 Batch 50 Loss 0.3588 Accuracy 0.3782\n",
      "Epoch 13 Batch 100 Loss 0.3614 Accuracy 0.3806\n",
      "Epoch 13 Batch 150 Loss 0.3609 Accuracy 0.3803\n",
      "Epoch 13 Batch 200 Loss 0.3644 Accuracy 0.3788\n",
      "Epoch 13 Batch 250 Loss 0.3678 Accuracy 0.3779\n",
      "Epoch 13 Batch 300 Loss 0.3709 Accuracy 0.3769\n",
      "Epoch 13 Batch 350 Loss 0.3739 Accuracy 0.3763\n",
      "Epoch 13 Batch 400 Loss 0.3757 Accuracy 0.3754\n",
      "Epoch 13 Batch 450 Loss 0.3778 Accuracy 0.3751\n",
      "Epoch 13 Batch 500 Loss 0.3804 Accuracy 0.3746\n",
      "Epoch 13 Batch 550 Loss 0.3827 Accuracy 0.3745\n",
      "Epoch 13 Batch 600 Loss 0.3853 Accuracy 0.3738\n",
      "Epoch 13 Batch 650 Loss 0.3881 Accuracy 0.3734\n",
      "Epoch 13 Batch 700 Loss 0.3912 Accuracy 0.3733\n",
      "Epoch 13 Loss 0.3914 Accuracy 0.3733\n",
      "Time taken for 1 epoch: 62.906967639923096 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.3208 Accuracy 0.3766\n",
      "Epoch 14 Batch 50 Loss 0.3588 Accuracy 0.3862\n",
      "Epoch 14 Batch 100 Loss 0.3541 Accuracy 0.3820\n",
      "Epoch 14 Batch 150 Loss 0.3558 Accuracy 0.3813\n",
      "Epoch 14 Batch 200 Loss 0.3578 Accuracy 0.3802\n",
      "Epoch 14 Batch 250 Loss 0.3590 Accuracy 0.3790\n",
      "Epoch 14 Batch 300 Loss 0.3618 Accuracy 0.3783\n",
      "Epoch 14 Batch 350 Loss 0.3653 Accuracy 0.3783\n",
      "Epoch 14 Batch 400 Loss 0.3677 Accuracy 0.3771\n",
      "Epoch 14 Batch 450 Loss 0.3707 Accuracy 0.3768\n",
      "Epoch 14 Batch 500 Loss 0.3732 Accuracy 0.3765\n",
      "Epoch 14 Batch 550 Loss 0.3761 Accuracy 0.3763\n",
      "Epoch 14 Batch 600 Loss 0.3793 Accuracy 0.3757\n",
      "Epoch 14 Batch 650 Loss 0.3812 Accuracy 0.3750\n",
      "Epoch 14 Batch 700 Loss 0.3847 Accuracy 0.3746\n",
      "Epoch 14 Loss 0.3847 Accuracy 0.3746\n",
      "Time taken for 1 epoch: 63.05089259147644 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.3303 Accuracy 0.3858\n",
      "Epoch 15 Batch 50 Loss 0.3465 Accuracy 0.3864\n",
      "Epoch 15 Batch 100 Loss 0.3491 Accuracy 0.3854\n",
      "Epoch 15 Batch 150 Loss 0.3492 Accuracy 0.3836\n",
      "Epoch 15 Batch 200 Loss 0.3530 Accuracy 0.3824\n",
      "Epoch 15 Batch 250 Loss 0.3537 Accuracy 0.3812\n",
      "Epoch 15 Batch 300 Loss 0.3555 Accuracy 0.3807\n",
      "Epoch 15 Batch 350 Loss 0.3582 Accuracy 0.3807\n",
      "Epoch 15 Batch 400 Loss 0.3608 Accuracy 0.3797\n",
      "Epoch 15 Batch 450 Loss 0.3630 Accuracy 0.3793\n",
      "Epoch 15 Batch 500 Loss 0.3659 Accuracy 0.3785\n",
      "Epoch 15 Batch 550 Loss 0.3679 Accuracy 0.3776\n",
      "Epoch 15 Batch 600 Loss 0.3698 Accuracy 0.3767\n",
      "Epoch 15 Batch 650 Loss 0.3722 Accuracy 0.3758\n",
      "Epoch 15 Batch 700 Loss 0.3747 Accuracy 0.3754\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-7\n",
      "Epoch 15 Loss 0.3749 Accuracy 0.3753\n",
      "Time taken for 1 epoch: 63.07877016067505 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.3292 Accuracy 0.3940\n",
      "Epoch 16 Batch 50 Loss 0.3314 Accuracy 0.3808\n",
      "Epoch 16 Batch 100 Loss 0.3358 Accuracy 0.3816\n",
      "Epoch 16 Batch 150 Loss 0.3392 Accuracy 0.3821\n",
      "Epoch 16 Batch 200 Loss 0.3419 Accuracy 0.3811\n",
      "Epoch 16 Batch 250 Loss 0.3453 Accuracy 0.3800\n",
      "Epoch 16 Batch 300 Loss 0.3493 Accuracy 0.3807\n",
      "Epoch 16 Batch 350 Loss 0.3515 Accuracy 0.3799\n",
      "Epoch 16 Batch 400 Loss 0.3543 Accuracy 0.3799\n",
      "Epoch 16 Batch 450 Loss 0.3569 Accuracy 0.3796\n",
      "Epoch 16 Batch 500 Loss 0.3604 Accuracy 0.3791\n",
      "Epoch 16 Batch 550 Loss 0.3636 Accuracy 0.3791\n",
      "Epoch 16 Batch 600 Loss 0.3655 Accuracy 0.3786\n",
      "Epoch 16 Batch 650 Loss 0.3673 Accuracy 0.3776\n",
      "Epoch 16 Batch 700 Loss 0.3690 Accuracy 0.3771\n",
      "Epoch 16 Loss 0.3690 Accuracy 0.3771\n",
      "Time taken for 1 epoch: 62.79835319519043 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.3806 Accuracy 0.4380\n",
      "Epoch 17 Batch 50 Loss 0.3312 Accuracy 0.3876\n",
      "Epoch 17 Batch 100 Loss 0.3308 Accuracy 0.3843\n",
      "Epoch 17 Batch 150 Loss 0.3360 Accuracy 0.3846\n",
      "Epoch 17 Batch 200 Loss 0.3383 Accuracy 0.3840\n",
      "Epoch 17 Batch 250 Loss 0.3405 Accuracy 0.3818\n",
      "Epoch 17 Batch 300 Loss 0.3435 Accuracy 0.3814\n",
      "Epoch 17 Batch 350 Loss 0.3450 Accuracy 0.3806\n",
      "Epoch 17 Batch 400 Loss 0.3471 Accuracy 0.3802\n",
      "Epoch 17 Batch 450 Loss 0.3494 Accuracy 0.3801\n",
      "Epoch 17 Batch 500 Loss 0.3525 Accuracy 0.3799\n",
      "Epoch 17 Batch 550 Loss 0.3544 Accuracy 0.3792\n",
      "Epoch 17 Batch 600 Loss 0.3572 Accuracy 0.3789\n",
      "Epoch 17 Batch 650 Loss 0.3593 Accuracy 0.3784\n",
      "Epoch 17 Batch 700 Loss 0.3618 Accuracy 0.3785\n",
      "Epoch 17 Loss 0.3620 Accuracy 0.3785\n",
      "Time taken for 1 epoch: 63.037699699401855 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.3435 Accuracy 0.3734\n",
      "Epoch 18 Batch 50 Loss 0.3286 Accuracy 0.3829\n",
      "Epoch 18 Batch 100 Loss 0.3336 Accuracy 0.3863\n",
      "Epoch 18 Batch 150 Loss 0.3338 Accuracy 0.3858\n",
      "Epoch 18 Batch 200 Loss 0.3330 Accuracy 0.3846\n",
      "Epoch 18 Batch 250 Loss 0.3348 Accuracy 0.3841\n",
      "Epoch 18 Batch 300 Loss 0.3374 Accuracy 0.3843\n",
      "Epoch 18 Batch 350 Loss 0.3399 Accuracy 0.3840\n",
      "Epoch 18 Batch 400 Loss 0.3420 Accuracy 0.3834\n",
      "Epoch 18 Batch 450 Loss 0.3437 Accuracy 0.3824\n",
      "Epoch 18 Batch 500 Loss 0.3464 Accuracy 0.3820\n",
      "Epoch 18 Batch 550 Loss 0.3485 Accuracy 0.3813\n",
      "Epoch 18 Batch 600 Loss 0.3497 Accuracy 0.3803\n",
      "Epoch 18 Batch 650 Loss 0.3525 Accuracy 0.3805\n",
      "Epoch 18 Batch 700 Loss 0.3548 Accuracy 0.3799\n",
      "Epoch 18 Loss 0.3549 Accuracy 0.3799\n",
      "Time taken for 1 epoch: 63.16640877723694 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.3179 Accuracy 0.4083\n",
      "Epoch 19 Batch 50 Loss 0.3216 Accuracy 0.3896\n",
      "Epoch 19 Batch 100 Loss 0.3198 Accuracy 0.3883\n",
      "Epoch 19 Batch 150 Loss 0.3213 Accuracy 0.3881\n",
      "Epoch 19 Batch 200 Loss 0.3252 Accuracy 0.3873\n",
      "Epoch 19 Batch 250 Loss 0.3264 Accuracy 0.3857\n",
      "Epoch 19 Batch 300 Loss 0.3296 Accuracy 0.3845\n",
      "Epoch 19 Batch 350 Loss 0.3330 Accuracy 0.3844\n",
      "Epoch 19 Batch 400 Loss 0.3354 Accuracy 0.3846\n",
      "Epoch 19 Batch 450 Loss 0.3384 Accuracy 0.3838\n",
      "Epoch 19 Batch 500 Loss 0.3407 Accuracy 0.3835\n",
      "Epoch 19 Batch 550 Loss 0.3428 Accuracy 0.3830\n",
      "Epoch 19 Batch 600 Loss 0.3449 Accuracy 0.3825\n",
      "Epoch 19 Batch 650 Loss 0.3473 Accuracy 0.3818\n",
      "Epoch 19 Batch 700 Loss 0.3492 Accuracy 0.3807\n",
      "Epoch 19 Loss 0.3491 Accuracy 0.3807\n",
      "Time taken for 1 epoch: 63.15117430686951 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.2812 Accuracy 0.3910\n",
      "Epoch 20 Batch 50 Loss 0.3143 Accuracy 0.3895\n",
      "Epoch 20 Batch 100 Loss 0.3150 Accuracy 0.3884\n",
      "Epoch 20 Batch 150 Loss 0.3165 Accuracy 0.3855\n",
      "Epoch 20 Batch 200 Loss 0.3214 Accuracy 0.3863\n",
      "Epoch 20 Batch 250 Loss 0.3224 Accuracy 0.3853\n",
      "Epoch 20 Batch 300 Loss 0.3259 Accuracy 0.3860\n",
      "Epoch 20 Batch 350 Loss 0.3282 Accuracy 0.3861\n",
      "Epoch 20 Batch 400 Loss 0.3306 Accuracy 0.3858\n",
      "Epoch 20 Batch 450 Loss 0.3315 Accuracy 0.3850\n",
      "Epoch 20 Batch 500 Loss 0.3342 Accuracy 0.3854\n",
      "Epoch 20 Batch 550 Loss 0.3359 Accuracy 0.3846\n",
      "Epoch 20 Batch 600 Loss 0.3375 Accuracy 0.3836\n",
      "Epoch 20 Batch 650 Loss 0.3400 Accuracy 0.3831\n",
      "Epoch 20 Batch 700 Loss 0.3426 Accuracy 0.3826\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-8\n",
      "Epoch 20 Loss 0.3426 Accuracy 0.3826\n",
      "Time taken for 1 epoch: 63.16035294532776 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # inp sentence is portuguese, hence adding the start and end token\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # as the target is english, the first word to the transformer should be the\n",
    "    # english start token.\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        # concatentate the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este  um problema que temos que resolver.\n",
      "Predicted translation: this is a problem that we have to deal with .\n",
      "Real translation: this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "translate(\"este  um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: os meus vizinhos ouviram sobre esta ideia.\n",
      "Predicted translation: my neighbors heard about this idea .\n",
      "Real translation: and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: este  o primeiro livro que eu fiz.\n",
      "Predicted translation: this is the first book i made .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0625e7c4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real translation: this is the first book i've ever done.\n"
     ]
    }
   ],
   "source": [
    "translate(\"este  o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
